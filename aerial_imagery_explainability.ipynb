{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nlim-uow/aerial_segmentation/blob/main/aerial_imagery_explainability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg6FvUp7hby5"
      },
      "source": [
        "#Import Libraries and install pytorch-grad-cam"
      ],
      "id": "qg6FvUp7hby5"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae066023",
        "outputId": "2ea2d954-146f-47ed-8ee1-113e79986d9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version:  1.12.1+cu113\n",
            "Torchvision Version:  0.13.1+cu113\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: grad-cam==1.3.7 in /usr/local/lib/python3.7/dist-packages (1.3.7)\n",
            "Requirement already satisfied: ttach in /usr/local/lib/python3.7/dist-packages (from grad-cam==1.3.7) (0.0.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from grad-cam==1.3.7) (4.6.0.66)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from grad-cam==1.3.7) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from grad-cam==1.3.7) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from grad-cam==1.3.7) (1.12.1+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from grad-cam==1.3.7) (4.64.1)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from grad-cam==1.3.7) (0.13.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.1->grad-cam==1.3.7) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.8.2->grad-cam==1.3.7) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.8.2->grad-cam==1.3.7) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.8.2->grad-cam==1.3.7) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.8.2->grad-cam==1.3.7) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.8.2->grad-cam==1.3.7) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\n",
        "from torchvision.transforms import ToTensor\n",
        "from scipy.special import softmax\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "from skimage.measure import label, regionprops\n",
        "\n",
        "module_path = os.path.abspath(os.path.join('..'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "!pip install grad-cam==1.3.7\n",
        "from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image"
      ],
      "id": "ae066023"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuvkhUjD_24R"
      },
      "source": [
        "#Helper Functions\n",
        "In the following blocks we have the helper functions for creating, training and visualizing the GradCam explanations for the classifications"
      ],
      "id": "RuvkhUjD_24R"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "b9abdda9"
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False, patience=5):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    early_stopping = 0\n",
        "    best_loss = np.Inf\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output. In train\n",
        "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                    #   but in testing we only consider the final output.\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "            \n",
        "            \n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "              best_acc = epoch_acc\n",
        "              best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "              val_acc_history.append(epoch_acc)\n",
        "            \n",
        "            if phase == 'val': \n",
        "              if epoch_loss > best_loss:\n",
        "                early_stopping = early_stopping + 1\n",
        "              else:\n",
        "                best_loss=epoch_loss\n",
        "                early_stopping = 0\n",
        "        if early_stopping>patience:\n",
        "            break\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ],
      "id": "b9abdda9"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "40610831"
      },
      "outputs": [],
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "      for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    else:\n",
        "      for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "    "
      ],
      "id": "40610831"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fa769728"
      },
      "outputs": [],
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True, visualization=False):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet18\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(weights='IMAGENET1K_V1')\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"resnet34\":\n",
        "        \"\"\" Resnet34\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet34(weights='IMAGENET1K_V1')\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"resnet50\":\n",
        "        \"\"\" Resnet50\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet50(weights='IMAGENET1K_V1')\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"resnet101\":\n",
        "        \"\"\" Resnet101\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet101(weights='IMAGENET1K_V1')\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "        \n",
        "    elif model_name == \"resnet152\":\n",
        "        \"\"\" Resnet152\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet152(weights='IMAGENET1K_V1')\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224    \n",
        "    \n",
        "    elif model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model_ft = models.alexnet(weights='IMAGENET1K_V1')\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg11_bn(weights='IMAGENET1K_V1')\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model_ft = models.squeezenet1_0(weights='IMAGENET1K_V1')\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(weights='IMAGENET1K_V1')\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"inception\":\n",
        "        \"\"\" Inception v3 \n",
        "        Be careful, expects (299,299) sized images and has auxiliary output\n",
        "        \"\"\"\n",
        "        model_ft = models.inception_v3(weights='IMAGENET1K_V1')\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        # Handle the auxilary net\n",
        "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        # Handle the primary net\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 299\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "    \n",
        "    return model_ft, input_size"
      ],
      "id": "fa769728"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tHqA-4ifXlIo"
      },
      "outputs": [],
      "source": [
        "def display_heatmap(img,model,cam,cls_list,true_label=0,th=0.75,oracle=False):\n",
        "    img=img.resize((224,224))\n",
        "    img_copy=np.array(img)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    ])\n",
        "    inv_normalize = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255],\n",
        "        std=[1/0.229, 1/0.224, 1/0.255])\n",
        "    ])\n",
        "    x = transform(img)\n",
        "    x = x.unsqueeze(0)\n",
        "    x_y=torch.flip(x,[3])\n",
        "    x_x=torch.flip(x,[2])\n",
        "    x_xy=torch.flip(x,[2,3])\n",
        "    x_rot90=torch.rot90(x,1,[2,3])\n",
        "    x_rot270=torch.rot90(x,3,[2,3])\n",
        "    x_x_rot90=torch.rot90(x_x,1,[2,3])\n",
        "    x_y_rot90=torch.rot90(x_y,1,[2,3])\n",
        "    predict=model.cuda()(x.cuda()).detach().cpu().numpy()\n",
        "    pred=np.argmax(predict)\n",
        "    if oracle==True:\n",
        "      predictedClass=[ClassifierOutputTarget(pred)]\n",
        "      predCls=cls_list[pred]\n",
        "      predScore=softmax(predict)[0,pred]\n",
        "    else:\n",
        "      predictedClass=[ClassifierOutputTarget(true_label)]\n",
        "      predCls=cls_list[true_label]\n",
        "      predScore=softmax(predict)[0,true_label]\n",
        "    if predScore<0.1:\n",
        "       print(f\"The confidence score of the classifier is less than 0.1 ({predScore:0.4}). It is possible that the network was unable to find the land feature in the image. \")\n",
        "       print(f\"The following heatmap and bounding box assumes that the network correctly identified the land feature.\")\n",
        "    grayscale_cam = cam(input_tensor=x,targets=predictedClass)[0]\n",
        "    grayscale_cam_x = cam(input_tensor=x_x,targets=predictedClass)[0]\n",
        "    grayscale_cam_y = cam(input_tensor=x_y,targets=predictedClass)[0]\n",
        "    grayscale_cam_xy = cam(input_tensor=x_xy,targets=predictedClass)[0]\n",
        "    grayscale_cam_x_rot90 = cam(input_tensor=x_rot90,targets=predictedClass)[0]\n",
        "    grayscale_cam_x_rot270 = cam(input_tensor=x_rot270,targets=predictedClass)[0]\n",
        "    grayscale_cam_x_x_rot90 = cam(input_tensor=x_x_rot90,targets=predictedClass)[0]\n",
        "    grayscale_cam_x_y_rot90 = cam(input_tensor=x_y_rot90,targets=predictedClass)[0]\n",
        "    grayscale_cam_ori = grayscale_cam\n",
        "    grayscale_cam_x=np.flip(grayscale_cam_x,0)\n",
        "    grayscale_cam_y=np.flip(grayscale_cam_y,1)\n",
        "    grayscale_cam_xy=np.flip(grayscale_cam_xy,[0,1])\n",
        "    grayscale_cam_x_rot90 = np.rot90(grayscale_cam_x_rot90,3,[0,1])\n",
        "    grayscale_cam_x_rot270 = np.rot90(grayscale_cam_x_rot270,1,[0,1])\n",
        "    grayscale_cam_x_x_rot90 = np.flip(np.rot90(grayscale_cam_x_x_rot90,3,[0,1]),0)\n",
        "    grayscale_cam_x_y_rot90 = np.flip(np.rot90(grayscale_cam_x_y_rot90,3,[0,1]),1)\n",
        "    grayscale_cam_max=np.maximum(grayscale_cam,grayscale_cam_x)\n",
        "    grayscale_cam_max=np.maximum(grayscale_cam_max,grayscale_cam_y)\n",
        "    grayscale_cam_max=np.maximum(grayscale_cam_max,grayscale_cam_xy)\n",
        "    grayscale_cam_max=np.maximum(grayscale_cam_max,grayscale_cam_x_rot90)\n",
        "    grayscale_cam_max=np.maximum(grayscale_cam_max,grayscale_cam_x_rot270)\n",
        "    grayscale_cam_max=np.maximum(grayscale_cam_max,grayscale_cam_x_x_rot90)\n",
        "    grayscale_cam_max=np.maximum(grayscale_cam_max,grayscale_cam_x_y_rot90)\n",
        "    grayscale_cam_avg=np.sum([grayscale_cam_ori,grayscale_cam_x,grayscale_cam_y,grayscale_cam_xy,grayscale_cam_x_rot90,grayscale_cam_x_rot270,grayscale_cam_x_x_rot90,grayscale_cam_x_y_rot90],axis=0)/8\n",
        "    thresholded_cam = grayscale_cam_max.copy()\n",
        "    cam_on_image = show_cam_on_image(np.array(img_copy) / 255, thresholded_cam,True,cv2.COLORMAP_HOT)\n",
        "    mask = thresholded_cam.copy()\n",
        "    mask[mask > th] = 1\n",
        "    box_candidates = regionprops(label(mask))\n",
        "    cambbox=np.zeros(mask.shape)\n",
        "    cambbox_list=[]\n",
        "    for box_candidate in box_candidates:\n",
        "        (y1,x1,y2,x2)=box_candidate.bbox\n",
        "        cambbox[y1:y2,x1:x2]=1\n",
        "    for box_candidate in box_candidates:\n",
        "        (y1,x1,y2,x2)=box_candidate.bbox\n",
        "        cv2.rectangle(cam_on_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "        cv2.putText(cam_on_image,f\"{cls_list[pred]}-{predScore:0.4}\", (x1+2, y1+25), cv2.FONT_HERSHEY_SIMPLEX, 0.33, (255, 255, 255), 1)\n",
        "    visualization = Image.fromarray(cam_on_image.astype(np.uint8))\n",
        "    new_im = Image.new('RGB', (448,224))\n",
        "    new_im.paste(img, (0,0))\n",
        "    new_im.paste(visualization, (224,0))\n",
        "    return new_im"
      ],
      "id": "tHqA-4ifXlIo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MQcdRHoAAIz"
      },
      "source": [
        "# Downloading the dataset\n",
        "In the following blocks we will download the dataset from https://datasets.cms.waikato.ac.nz/taiao/data/waikato_aerial_imagery_2017 \n",
        "The tar file contains 13 classes of land types as defined by LCDB4.0, there are 666 images for each class in the training set and 334 images in the validation/hold-out set.\n",
        "\n",
        "The names of the class are taken from the folder names"
      ],
      "id": "_MQcdRHoAAIz"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOaYYe9SU_xB",
        "outputId": "8db0821d-5264-4fb7-fb9d-55a28a38ec15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-30 21:02:57--  https://datasets.cms.waikato.ac.nz/taiao/data/waikato_aerial_imagery_2017/classification.tar\n",
            "Resolving datasets.cms.waikato.ac.nz (datasets.cms.waikato.ac.nz)... 130.217.218.32\n",
            "Connecting to datasets.cms.waikato.ac.nz (datasets.cms.waikato.ac.nz)|130.217.218.32|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1105899520 (1.0G) [application/x-tar]\n",
            "Saving to: ‘classification.tar’\n",
            "\n",
            "classification.tar  100%[===================>]   1.03G  13.9MB/s    in 77s     \n",
            "\n",
            "2022-11-30 21:04:16 (13.6 MB/s) - ‘classification.tar’ saved [1105899520/1105899520]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Download Dataset from dataset.cms.waikato.ac.nz\n",
        "!mkdir results\n",
        "!mkdir -p data/aerial_imagery/classification\n",
        "!wget https://datasets.cms.waikato.ac.nz/taiao/data/waikato_aerial_imagery_2017/classification.tar\n"
      ],
      "id": "uOaYYe9SU_xB"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dUAjA96AWhpf"
      },
      "outputs": [],
      "source": [
        "!tar -xf classification.tar --directory /content/data/aerial_imagery/"
      ],
      "id": "dUAjA96AWhpf"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7-D9DnoWpre",
        "outputId": "8406f752-f799-4a66-da90-7d85e5a6f503"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/results\n"
          ]
        }
      ],
      "source": [
        "%cd /content/results"
      ],
      "id": "E7-D9DnoWpre"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z02XaMauA2PH"
      },
      "source": [
        "#Training the model\n",
        "The following code initializes the model and does transfer learning "
      ],
      "id": "Z02XaMauA2PH"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cea5715",
        "outputId": "e578e3e8-e96e-4600-c798-52f9a609034f",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Datasets and Dataloaders...\n",
            "Params to learn:\n",
            "\t fc.weight\n",
            "\t fc.bias\n",
            "Epoch 0/9\n",
            "----------\n",
            "train Loss: 2.2517 Acc: 0.2471\n",
            "val Loss: 2.0784 Acc: 0.2950\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n",
            "train Loss: 1.9597 Acc: 0.3399\n",
            "val Loss: 1.9455 Acc: 0.3349\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n",
            "train Loss: 1.8461 Acc: 0.3831\n",
            "val Loss: 1.8988 Acc: 0.3526\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n",
            "train Loss: 1.7940 Acc: 0.3928\n",
            "val Loss: 1.8606 Acc: 0.3690\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n",
            "train Loss: 1.7626 Acc: 0.4029\n",
            "val Loss: 1.8498 Acc: 0.3694\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n",
            "train Loss: 1.7344 Acc: 0.4161\n",
            "val Loss: 1.8322 Acc: 0.3809\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n",
            "train Loss: 1.7198 Acc: 0.4166\n",
            "val Loss: 1.8245 Acc: 0.3816\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n",
            "train Loss: 1.7012 Acc: 0.4271\n",
            "val Loss: 1.8243 Acc: 0.3819\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n",
            "train Loss: 1.6928 Acc: 0.4353\n",
            "val Loss: 1.8035 Acc: 0.3943\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n",
            "train Loss: 1.6747 Acc: 0.4351\n",
            "val Loss: 1.8097 Acc: 0.3867\n",
            "\n",
            "Training complete in 7m 22s\n",
            "Best val Acc: 0.394288\n",
            "Epoch 0/29\n",
            "----------\n",
            "train Loss: 1.6220 Acc: 0.4551\n",
            "val Loss: 1.7066 Acc: 0.4357\n",
            "\n",
            "Epoch 1/29\n",
            "----------\n",
            "train Loss: 1.4905 Acc: 0.4968\n",
            "val Loss: 1.6520 Acc: 0.4466\n",
            "\n",
            "Epoch 2/29\n",
            "----------\n",
            "train Loss: 1.4072 Acc: 0.5267\n",
            "val Loss: 1.6164 Acc: 0.4625\n",
            "\n",
            "Epoch 3/29\n",
            "----------\n",
            "train Loss: 1.3380 Acc: 0.5501\n",
            "val Loss: 1.5929 Acc: 0.4714\n",
            "\n",
            "Epoch 4/29\n",
            "----------\n",
            "train Loss: 1.2668 Acc: 0.5698\n",
            "val Loss: 1.5799 Acc: 0.4749\n",
            "\n",
            "Epoch 5/29\n",
            "----------\n",
            "train Loss: 1.2139 Acc: 0.5952\n",
            "val Loss: 1.5734 Acc: 0.4869\n",
            "\n",
            "Epoch 6/29\n",
            "----------\n",
            "train Loss: 1.1688 Acc: 0.6098\n",
            "val Loss: 1.5636 Acc: 0.4912\n",
            "\n",
            "Epoch 7/29\n",
            "----------\n",
            "train Loss: 1.1161 Acc: 0.6264\n",
            "val Loss: 1.5554 Acc: 0.4945\n",
            "\n",
            "Epoch 8/29\n",
            "----------\n",
            "train Loss: 1.0852 Acc: 0.6407\n",
            "val Loss: 1.5469 Acc: 0.4984\n",
            "\n",
            "Epoch 9/29\n",
            "----------\n",
            "train Loss: 1.0276 Acc: 0.6623\n",
            "val Loss: 1.5611 Acc: 0.4919\n",
            "\n",
            "Epoch 10/29\n",
            "----------\n",
            "train Loss: 0.9910 Acc: 0.6733\n",
            "val Loss: 1.5686 Acc: 0.4894\n",
            "\n",
            "Epoch 11/29\n",
            "----------\n",
            "train Loss: 0.9410 Acc: 0.6945\n",
            "val Loss: 1.5696 Acc: 0.4940\n",
            "\n",
            "Epoch 12/29\n",
            "----------\n",
            "train Loss: 0.9019 Acc: 0.7108\n",
            "val Loss: 1.5744 Acc: 0.4924\n",
            "\n",
            "Epoch 13/29\n",
            "----------\n",
            "train Loss: 0.8588 Acc: 0.7309\n",
            "val Loss: 1.5880 Acc: 0.4949\n",
            "\n",
            "Epoch 14/29\n",
            "----------\n",
            "train Loss: 0.8236 Acc: 0.7412\n",
            "val Loss: 1.5891 Acc: 0.4982\n",
            "Training complete in 12m 1s\n",
            "Best val Acc: 0.498388\n"
          ]
        }
      ],
      "source": [
        "data_dir = \"/content/data/aerial_imagery/classification\"\n",
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
        "model_names=['resnet18']\n",
        "\n",
        "batch_size = 32\n",
        "input_size = 224\n",
        "pretrain_epochs = 10 # change to a larger number if the model accuracy is insufficient \n",
        "finetune_epochs = 30 # change to a larger number if the model accuracy is insufficient\n",
        "runCount=1\n",
        "feature_extract = True # Sets up transfer learning\n",
        "\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "#        transforms.RandomApply([transforms.RandomRotation((90, 90))], p=0.5),\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "# Create training and validation datasets\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
        "\n",
        "clsList,clsIdx=image_datasets['train'].find_classes(f'{data_dir}/train')\n",
        "num_classes=len(clsList)\n",
        "\n",
        "for runNo in range(runCount):\n",
        "    for model_name in model_names:\n",
        "        model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "        model_ft = model_ft.to(device)\n",
        "\n",
        "        params_to_update = model_ft.parameters()\n",
        "        print(\"Params to learn:\")\n",
        "        if feature_extract:\n",
        "            params_to_update = []\n",
        "            for name,param in model_ft.named_parameters():\n",
        "                if param.requires_grad == True:\n",
        "                    params_to_update.append(param)\n",
        "                    print(\"\\t\",name)\n",
        "        else:\n",
        "            for name,param in model_ft.named_parameters():\n",
        "                if param.requires_grad == True:\n",
        "                    print(\"\\t\",name)\n",
        "\n",
        "        optimizer_pt = optim.Adam(params_to_update, lr=0.0003)\n",
        "        optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.00001)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_pt, num_epochs=pretrain_epochs, is_inception=(model_name==\"inception\"))\n",
        "        set_parameter_requires_grad(model_ft, False)\n",
        "        model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=finetune_epochs, is_inception=(model_name==\"inception\"))\n",
        "\n",
        "        torch.save(model_ft.state_dict(), f\"aerial_small_auged_noshift_norot_{model_name}_{runNo}.pth\")"
      ],
      "id": "2cea5715"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWGcaGeMFz5A"
      },
      "source": [
        "#Visualize the gradcam explanation\n",
        "The following block of code visualizes the gradcam activation on validation/holdout set.  "
      ],
      "id": "sWGcaGeMFz5A"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7glq6rTLBGRc"
      },
      "outputs": [],
      "source": [
        "data_dir = \"/content/data/aerial_imagery/classification\"\n",
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
        "model_names=['resnet18']\n",
        "runCount=1\n",
        "val_data=datasets.ImageFolder(os.path.join(data_dir, 'val'))\n",
        "train_datasets = datasets.ImageFolder(os.path.join(data_dir, 'train'))\n",
        "\n",
        "clsList,clsIdx=train_datasets.find_classes(os.path.join(data_dir, 'train'))\n",
        "num_classes=len(clsList)\n",
        "for model_name in model_names:\n",
        "  for runNo in range(runCount):\n",
        "    model_infer,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=True)\n",
        "    model_infer.load_state_dict(torch.load(f'aerial_small_auged_noshift_norot_{model_name}_{runNo}.pth'))\n",
        "    model_infer.eval()\n",
        "    folder_name = f\"/content/results/aerial-imagery-visualization_{runNo}\" ## Output Directory\n",
        "    Path(folder_name).mkdir(parents=True, exist_ok=True)\n",
        "    target_layer = [model_infer.layer4[-1]]\n",
        "    val_data=datasets.ImageFolder(os.path.join(data_dir, 'val'))\n",
        "    cam = GradCAM(model=model_infer, target_layers=target_layer,use_cuda=True)\n",
        "    for classes in clsList:\n",
        "      Path(os.path.join(folder_name, classes)).mkdir(parents=True, exist_ok=True)\n",
        "  \n",
        "    for i in range(0,4342):\n",
        "      imgNo=i\n",
        "      (img,true_label)=val_data.__getitem__(imgNo)\n",
        "      visualization_im=display_heatmap(img,model_infer,cam,clsList,true_label,th=0.75,oracle=True) ## Oracle assumes we know the true label rather than using the predicted label to report the GradCam Activation \n",
        "      visualization_im.save(f'{folder_name}/{clsList[true_label]}/{i}.png')"
      ],
      "id": "7glq6rTLBGRc"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "RWn_KIPoBzXk"
      },
      "outputs": [],
      "source": [],
      "id": "RWn_KIPoBzXk"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "qg6FvUp7hby5"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}